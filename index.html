<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Huang(Raven) Huang</title>
  
  <meta name="author" content="Huang(Raven) Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Huang(Raven) Huang</name>
              </p>
              <p>I am a final year PhD student in EECS at UC Berkeley, advised by Prof. Ken Goldberg. My research focuses on general robot learning. Recently I have worked on the in-context learning of robotics, the alignment between tactile, vision and language and the whole body control of a quadruped. 
              </p>
              <p style="text-align:center">
                <a href="mailto:huangr@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=N_KDBGcAAAAJ">Google Scholar</a> 
                <!-- &nbsp/&nbsp -->
              </p>
            </td>
            <td style="padding:2.5%;width:100%;max-width:100%">
              <a href="images/selfie.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/selfie.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 


          <!-- ICRT-->
          <tr onmouseout="icrt_stop()" onmouseover="icrt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <video  width=115% height=100% muted autoplay loop id="icrt">
                  <source src="images/teaser_icrt.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function icrt_start() {
                  document.getElementById('icrt_splash').style.opacity = "0";
                  document.getElementById('icrt_under').style.opacity = "1";
                }

                function icrt_stop() {
                  document.getElementById('icrt_splash').style.opacity = "1";
                  document.getElementById('icrt_under').style.opacity = "0";
                }
                icrt_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle>In-Context Imitation Learning via Next-Token Prediction</papertitle>
              <br>
              <!-- AUTHORS -->
              Letian Fu*,  <b>Huang Huang</b>*, Gaurav Datta*, Lawrence Yunliang Chen, William Chung-Ho Panitch, Fangchen Liu, Hui Li, Ken Goldberg <small>*Equal contribution</small>
              <br>
              <!-- CONFERENCE -->
              <em> arXiv preprint arXiv:2408.15980</em> 2024, <a href="https://icrt.dev/">Website</a>
              <p></p>
              <!-- SUMMARY -->
              We explore how to enhance next-token prediction models to perform in-context imitation learning on a real robot. We propose In-Context Robot Transformer (ICRT), a causal transformer that generalizes to unseen tasks conditioned on prompts of sensorimotor trajectories of the new task composing of image observations, actions and states tuples, collected through human teleoperation.
            </td>
          </tr>
          

          <!-- TVL-->
          <tr onmouseout="tvl_stop()" onmouseover="tvl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <img src="images/tvl.png" width='180' id='tvl_splash'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function tvl_start() {
                  document.getElementById('tvl_splash').style.opacity = "0";
                  document.getElementById('tvl_under').style.opacity = "1";
                }

                function tvl_stop() {
                  document.getElementById('tvl_splash').style.opacity = "1";
                  document.getElementById('tvl_under').style.opacity = "0";
                }
                tvl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle>A touch, vision, and language dataset for multimodal alignment</papertitle>
              <br>
              <!-- AUTHORS -->
              Letian Fu, Gaurav Datta*, <b>Huang Huang*</b>, William Chung-Ho Panitch*, Jaimyn Drake*, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta, Roberto Calandra, Ken Goldberg <small>*Equal contribution</small>
              <br>
              <!-- CONFERENCE -->
              <em> ICML</em> 2024, <b>Oral Presentation</b>, <a href="https://tactile-vlm.github.io/">Website</a>
              <p></p>
              <!-- SUMMARY -->
              We introduce the Touch-Vision-Language (TVL) dataset, which combines paired tactile and visual observations with both human-annotated and VLM-generated tactile-semantic labels. We then leverage a contrastive learning approach to train a CLIP-aligned tactile encoder and finetune an open-source LLM for a tactile description task. Our results show that incorporating tactile information allows us to significantly outperform state-of-the-art VLMs (including the label generating model) on a tactile understanding task.
            </td>
          </tr>


          <!-- Dog-->
          <tr onmouseout="dog_stop()" onmouseover="dog_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <video  width=115% height=100% muted autoplay loop id="icrt">
                  <source src="images/dog.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>

              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function dog_start() {
                  document.getElementById('dog_splash').style.opacity = "0";
                  document.getElementById('dog_under').style.opacity = "1";
                }

                function dog_stop() {
                  document.getElementById('dog_splash').style.opacity = "1";
                  document.getElementById('dog_under').style.opacity = "0";
                }
                dog_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle>Manipulator as a Tail: Promoting Dynamic Stability for Legged Locomotion</papertitle>
              <br>
              <!-- AUTHORS -->
              Huang Huang, Antonio Loquercio, Ashish Kumar, Neerja Thakkar, Ken Goldberg, Jitendra Malik
              <br>
              <!-- CONFERENCE -->
              <em> ICRA</em> 2024, <a href="https://sites.google.com/view/chimeralocomotion/home?authuser=1&pli=1">Website</a>
              <p></p>
              <!-- SUMMARY -->
              For locomotion, is an arm on a legged robot a liability or an asset for locomotion? Biological systems evolved additional limbs beyond legs that facilitates postural control. This work shows how a manipulator can be an asset for legged locomotion at high speeds or under external perturbations, where the arm serves beyond manipulation.
            </td>
          </tr>
          
          


           <!-- TACVIS -->
           <tr onmouseout="tacvis_stop()" onmouseover="tacvis_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='tacvis_under'>
                  <img src="images/tacvis_under.png" width='180'/>
                </div>
                <img src="images/tacvis_splash.png" width='180' id='tacvis_splash'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function tacvis_start() {
                  document.getElementById('tacvis_splash').style.opacity = "0";
                  document.getElementById('tacvis_under').style.opacity = "1";
                }

                function tacvis_stop() {
                  document.getElementById('tacvis_splash').style.opacity = "1";
                  document.getElementById('tacvis_under').style.opacity = "0";
                }
                tacvis_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle>Learning Self-Supervised Representations from Vision and Touch
                  for Active Sliding Perception of Deformable Surfaces</papertitle>
              <br>
              <!-- AUTHORS -->
              Justin Kerr*, <b>Huang Huang</b>*, Albert Wilcox, Ryan Hoque, Jeffrey Ichnowski, Roberto Calandra, and Ken Goldberg,
              <small>*Equal contribution</small>
              <br>
              <!-- CONFERENCE -->
              <em>RSS</em> 2023, <a href="https://arxiv.org/pdf/2209.13042">Paper</a>
              <p></p>
              <!-- SUMMARY -->
              We learn a self-supervised representation cross tactile and vistion using contrastive loss. We collect vision-tacitile pairs in a self-supervised way in real. The learned representation is utilized in the downstream active perception tasks without fine-tuning.
            </td>
          </tr>



          <!-- EVO-NERF -->
          <tr onmouseout="evonerf_stop()" onmouseover="evonerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='evonerf_under'>
                  <img src="images/evonerf_early_stop.png" width='180'/>
                </div>
                <video  width=115% height=100% muted autoplay loop id="evonerf_splash">
                  <source src="images/evonerf_splash.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function evonerf_start() {
                  document.getElementById('evonerf_splash').style.opacity = "0";
                  document.getElementById('evonerf_under').style.opacity = "1";
                }

                function evonerf_stop() {
                  document.getElementById('evonerf_splash').style.opacity = "1";
                  document.getElementById('evonerf_under').style.opacity = "0";
                }
                evonerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <a href="https://sites.google.com/view/evo-nerf">
                <papertitle>Evo-NeRF: Evolving NeRF for Sequential Robot Grasping</papertitle>
              </a>
              <br>
              <!-- AUTHORS -->
              Justin Kerr, Letian Fu, <b>Huang Huang</b>, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, Ken Goldberg
              <br>
              <!-- CONFERENCE -->
              <em>CoRL</em> 2022, <b>Oral Presentation</b>, <a href="https://openreview.net/forum?id=Bxr45keYrf">OpenReview</a>
              <p></p>
              <!-- SUMMARY -->
              We propose Evo-NeRF with additional geometry regularizations improving performance in rapid capture settings to achieve real-time, updateable scene reconstruction for rapidly grasping table-top transparent objects.
              We train a NeRF-adapted grasping network learns to ignore floaters.
            </td>
          </tr>

          <!-- prc -->
          <tr onmouseout="prc_stop()" onmouseover="prc_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='prc_splash'>
                  <img src="images/prc_teaser.jpg" width='180'/>
                </div>
                <video  width=115% height=100% muted autoplay loop id="prc_vid">
                  <source src="images/prc_tossvideo.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function prc_start() {
                  document.getElementById('prc_splash').style.opacity = "0";
                  document.getElementById('prc_vid').style.opacity = "1";
                }

                function prc_stop() {
                  document.getElementById('prc_splash').style.opacity = "1";
                  document.getElementById('prc_vid').style.opacity = "0";
                }
                prc_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->  
                <a href="https://sites.google.com/view/dynamic-cable">
                <papertitle>Real2Sim2Real: Self-Supervised Learning of Physical
                  Single-Step Dynamic Actions for Planar Robot Casting</papertitle>
                </a>
              <br>
              <!-- AUTHORS -->
              Vincent Lim*, <b>Huang Huang</b>*, Lawrence Yunliang Chen, Jonathan Wang,
              Jeffrey Ichnowski, Daniel Seita, Michael Laskey, Ken Goldberg, <small>*Equal contribution</small>
              <br>
              <!-- CONFERENCE -->
              <em>ICRA</em> 2021, <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9811651">paper</a>
              <p></p>
              <!-- SUMMARY -->
              We collect planar robot casting data in real in a self-supervised way to tune the simulation in Isaac Gym. We then collect more data in the tuned simulator. 
              Combined with upsampled real data, we learn a policy for planar robot casting to reach to a given target, attaining median error distance (as % of cable length) ranging
              from 8% to 14%.
            </td>
          </tr>
          
          <!-- acc -->
          

        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
